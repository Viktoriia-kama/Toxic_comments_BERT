# -*- coding: utf-8 -*-
"""EDA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/Viktoriia-kama/Toxic_comments_BERT/blob/main/EDA.ipynb

# Toxic Comment Classification

## Packages Loading
"""

import pandas as pd
import numpy as np


import matplotlib.pyplot as plt
import seaborn as sns

from google.colab import drive
import zipfile

from wordcloud import WordCloud

import nltk
nltk.download('wordnet')


from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.corpus import stopwords
import re
import string

"""## Data Preprocessing and Exploratory Data Analyasis

<a id='Data-loading'></a>
### üì• Data Loading
"""

train = pd.read_csv("/content/train.csv")
test = pd.read_csv("/content/test.csv")
test_y = pd.read_csv("/content/test_labels.csv")

"""### Data Analysis"""

train.head()

train.describe()

train['toxic'].value_counts(normalize = True)

"""–ù–∞—à –¥–∞—Ç–∞—Å–µ—Ç –º–∞—î –∑–Ω–∞—á–Ω–∏–π –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—ñ–≤. –¢–æ–∫—Å–∏—á–Ω—ñ –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ —Å–∫–ª–∞–¥–∞—é—Ç—å –º–µ–Ω—à–µ 10% –≤—ñ–¥ —É—Å—ñ—Ö –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤, —Ç–æ–¥—ñ —è–∫ –Ω–µ—Ç–æ–∫—Å–∏—á–Ω—ñ –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ —Å—Ç–∞–Ω–æ–≤–ª—è—Ç—å –ø–æ–Ω–∞–¥ 90%. –¶–µ –º–æ–∂–µ —É—Å–∫–ª–∞–¥–Ω–∏—Ç–∏ –Ω–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ, –æ—Å–∫—ñ–ª—å–∫–∏ –º–æ–¥–µ–ª—å –º–æ–∂–µ –º–∞—Ç–∏ —Ç–µ–Ω–¥–µ–Ω—Ü—ñ—é –¥–æ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –±—ñ–ª—å—à–æ—Å—Ç—ñ –∫–ª–∞—Å—É (–Ω–µ—Ç–æ–∫—Å–∏—á–Ω–∏–π) –±–µ–∑ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è –Ω–∞ –º–µ–Ω—à —á–∏—Å–µ–ª—å–Ω–æ–º—É –∫–ª–∞—Å—ñ (—Ç–æ–∫—Å–∏—á–Ω–∏–π).

–†–æ–∑–≥–ª—è–Ω—å–º–æ –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—é —Ä–æ–∑–ø–æ–¥—ñ–ª—É —Ç–æ–∫—Å–∏—á–Ω–∏—Ö —ñ –Ω–µ—Ç–æ–∫—Å–∏—á–Ω–∏—Ö –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤, —â–æ–± –∫—Ä–∞—â–µ –∑—Ä–æ–∑—É–º—ñ—Ç–∏ –¥–∞–Ω—ñ. –¢–∞–∫–æ–∂ –ø—Ä–æ–∞–Ω–∞–ª—ñ–∑—É—î–º–æ, —á–∏ —î —è–∫—ñ—Å—å —Å–ø–µ—Ü–∏—Ñ—ñ—á–Ω—ñ –ø–∞—Ç–µ—Ä–Ω–∏ –∞–±–æ —Ç–µ–º–∏ –≤ —Ç–æ–∫—Å–∏—á–Ω–∏—Ö –∫–æ–º–µ–Ω—Ç–∞—Ä—è—Ö, —è–∫—ñ –º–æ–∂–Ω–∞ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –¥–ª—è –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ
"""

plt.figure(figsize=(8, 6))
sns.countplot(x='toxic', data=train, palette='viridis')
plt.title('–†–æ–∑–ø–æ–¥—ñ–ª —Ç–æ–∫—Å–∏—á–Ω–∏—Ö —ñ –Ω–µ—Ç–æ–∫—Å–∏—á–Ω–∏—Ö –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤')
plt.xlabel('–¢–æ–∫—Å–∏—á–Ω—ñ—Å—Ç—å')
plt.ylabel('–ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤')
plt.xticks(ticks=[0, 1], labels=['–ù–µ—Ç–æ–∫—Å–∏—á–Ω–∏–π', '–¢–æ–∫—Å–∏—á–Ω–∏–π'])
plt.show()

test.head()

test_y.head()

"""–ó–Ω–∞—á–µ–Ω–Ω—è -1 –≤–∫–∞–∑—É—î –Ω–∞ —Ç–µ, —â–æ –º—ñ—Ç–∫–∞ –¥–ª—è —Ü—å–æ–≥–æ –∫–æ–º–µ–Ω—Ç–∞—Ä—è –∞–±–æ –Ω–µ –±—É–ª–∞ –Ω–∞–¥–∞–Ω–∞, –∞–±–æ –≤–æ–Ω–∞ —î –Ω–µ–¥—ñ–π—Å–Ω–æ—é. –¶–µ –º–æ–∂–µ –±—É—Ç–∏ —Å–∏–≥–Ω–∞–ª–æ–º, —â–æ —Ü—ñ –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ –∞–±–æ –Ω–µ –±—É–ª–∏ —Ä–æ–∑–º—ñ—á–µ–Ω—ñ, –∞–±–æ —Ä–æ–∑–º—ñ—Ç–∫–∞ –≤—ñ–¥—Å—É—Ç–Ω—è."""

test_y.describe()

test_y[test_y['toxic']==-1]

"""–í–∞–∂–ª–∏–≤–æ –≤–∏–∑–Ω–∞—á–∏—Ç–∏, —á–æ–º—É –≤ —Ç–µ—Å—Ç–æ–≤–æ–º—É –Ω–∞–±–æ—Ä—ñ —î –∑–Ω–∞—á–µ–Ω–Ω—è -1. –¶–µ –º–æ–∂—É—Ç—å –±—É—Ç–∏ –Ω–µ–ø–æ–≤–Ω—ñ –∞–±–æ –ø–æ—à–∫–æ–¥–∂–µ–Ω—ñ –¥–∞–Ω—ñ. –ë—É–ª–æ –≤–∏—Ä—ñ—à–µ–Ω–æ, —è–∫ –∑ –Ω–∏–º–∏ —Å–ø—Ä–∞–≤–∏—Ç–∏—Å—è ‚Äî ------------------

"""

test_y.toxic.unique()

"""–î–ª—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó –±—É–ª–æ —Å—Ç–≤–æ—Ä–µ–Ω–æ –≥—Ä–∞—Ñ—ñ–∫–∏, —è–∫—ñ –ø–æ—Ä—ñ–≤–Ω—é—é—Ç—å –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ä—è–¥–∫—ñ–≤ –∑ –∑–Ω–∞—á–µ–Ω–Ω—è–º–∏ -1, 0, —ñ 1 —É —Ç–µ—Å—Ç–æ–≤–æ–º—É –Ω–∞–±–æ—Ä—ñ –¥–∞–Ω–∏—Ö, —â–æ–± –∫—Ä–∞—â–µ –∑—Ä–æ–∑—É–º—ñ—Ç–∏ —ó—Ö —Ä–æ–∑–ø–æ–¥—ñ–ª —ñ –ø–æ—Ç–µ–Ω—Ü—ñ–π–Ω–∏–π –≤–ø–ª–∏–≤ –Ω–∞ –Ω–∞—à—É –º–æ–¥–µ–ª—å."""

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –∫—Ä—É–≥–æ–≤–æ—ó –¥—ñ–∞–≥—Ä–∞–º–∏ –¥–ª—è –æ–∑–Ω–∞–∫–∏ 'toxic'
toxic_counts = test_y['toxic'].value_counts()

plt.figure(figsize=(8, 8))
plt.pie(toxic_counts, labels=['-1 (–≤—ñ–¥—Å—É—Ç–Ω—ñ–π)', '0 (–Ω–µ—Ç–æ–∫—Å–∏—á–Ω–∏–π)', '1 (—Ç–æ–∫—Å–∏—á–Ω–∏–π)'], autopct='%1.1f%%', colors=['#FF9999', '#66B2FF', '#99FF99'])
plt.title('–†–æ–∑–ø–æ–¥—ñ–ª –∑–Ω–∞—á–µ–Ω—å –¥–ª—è –æ–∑–Ω–∞–∫–∏ "toxic" –≤ —Ç–µ—Å—Ç–æ–≤–æ–º—É –Ω–∞–±–æ—Ä—ñ')
plt.show()

"""1. –í–∏—Å–æ–∫–∏–π —Ä—ñ–≤–µ–Ω—å –≤—ñ–¥—Å—É—Ç–Ω—ñ—Ö –¥–∞–Ω–∏—Ö:

58% –∑–Ω–∞—á–µ–Ω—å -1 —É –Ω–∞—à–æ–º—É —Ç–µ—Å—Ç–æ–≤–æ–º—É –Ω–∞–±–æ—Ä—ñ –æ–∑–Ω–∞—á–∞—î, —â–æ –ø–æ–Ω–∞–¥ –ø–æ–ª–æ–≤–∏–Ω–∞ –¥–∞–Ω–∏—Ö –º–∞—î –Ω–µ–≤–∏–∑–Ω–∞—á–µ–Ω—ñ –∞–±–æ –≤—ñ–¥—Å—É—Ç–Ω—ñ –º—ñ—Ç–∫–∏. –¶–µ –º–æ–∂–µ –±—É—Ç–∏ —á–µ—Ä–µ–∑ –ø–æ–º–∏–ª–∫–∏ –≤ —Ä–æ–∑–º—ñ—Ç—Ü—ñ –∞–±–æ –Ω–µ–ø–æ–≤–Ω—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é. –¶–µ–π –≤–∏—Å–æ–∫–∏–π –≤—ñ–¥—Å–æ—Ç–æ–∫ –º–æ–∂–µ —Å—É—Ç—Ç—î–≤–æ –≤–ø–ª–∏–Ω—É—Ç–∏ –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –º–æ–¥–µ–ª—ñ, –æ—Å–∫—ñ–ª—å–∫–∏ –º–∏ –Ω–µ –∑–º–æ–∂–µ–º–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ —Ü—ñ —Ä—è–¥–∫–∏ –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ –º–æ–¥–µ–ª—ñ.
2. –ù–∏–∑—å–∫–∞ —á–∞—Å—Ç–∫–∞ —Ç–æ–∫—Å–∏—á–Ω–∏—Ö –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤:

–õ–∏—à–µ 4% –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤ —î —Ç–æ–∫—Å–∏—á–Ω–∏–º–∏ (–∑–Ω–∞—á–µ–Ω–Ω—è 1). –¶–µ –ø–æ–∫–∞–∑—É—î, —â–æ —Ç–µ—Å—Ç–æ–≤–∏–π –Ω–∞–±—ñ—Ä –º—ñ—Å—Ç–∏—Ç—å –¥—É–∂–µ –Ω–µ–≤–µ–ª–∏–∫—É –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–æ–∫—Å–∏—á–Ω–∏—Ö –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤. –¶–µ –º–æ–∂–µ –±—É—Ç–∏ –ø—Ä–æ–±–ª–µ–º–æ—é, —è–∫—â–æ –º–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ –Ω–∞–≤—á–µ–Ω–∞ –Ω–∞ —Ç–æ–∫—Å–∏—á–Ω–∏—Ö –∫–æ–º–µ–Ω—Ç–∞—Ä—è—Ö –∞–±–æ —è–∫—â–æ –≤ —Ç–µ—Å—Ç–æ–≤–æ–º—É –Ω–∞–±–æ—Ä—ñ –Ω–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–æ —Ç–æ–∫—Å–∏—á–Ω—ñ –ø—Ä–∏–∫–ª–∞–¥–∏.
3. –ë—ñ–ª—å—à—ñ—Å—Ç—å –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤ —î –Ω–µ—Ç–æ–∫—Å–∏—á–Ω–∏–º–∏:

37% –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤ —î –Ω–µ—Ç–æ–∫—Å–∏—á–Ω–∏–º–∏ (–∑–Ω–∞—á–µ–Ω–Ω—è 0). –¶–µ –æ–∑–Ω–∞—á–∞—î, —â–æ –∑–Ω–∞—á–Ω–∞ —á–∞—Å—Ç–∏–Ω–∞ —Ç–µ—Å—Ç–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö –º—ñ—Å—Ç–∏—Ç—å –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ, —è–∫—ñ –≤–≤–∞–∂–∞—é—Ç—å—Å—è –Ω–µ—Ç–æ–∫—Å–∏—á–Ω–∏–º–∏. –¶–µ —Ç–∞–∫–æ–∂ –≤–∫–∞–∑—É—î –Ω–∞ –¥–∏—Å–±–∞–ª–∞–Ω—Å –º—ñ–∂ –∫–ª–∞—Å–∞–º–∏.
"""

train.shape

test.shape

"""1. –†—ñ–∑–Ω–∏—Ü—è –≤ –∫—ñ–ª—å–∫–æ—Å—Ç—ñ —Ä—è–¥–∫—ñ–≤:

–¢–µ—Å—Ç–æ–≤–∏–π –Ω–∞–±—ñ—Ä –º–µ–Ω—à–∏–π –∑–∞ –Ω–∞–≤—á–∞–ª—å–Ω–∏–π (159,571 –ø—Ä–æ—Ç–∏ 153,164). –¶–µ –Ω–æ—Ä–º–∞–ª—å–Ω–∞ —Å–∏—Ç—É–∞—Ü—ñ—è, –∞–ª–µ –º–∏ –ø–æ–≤–∏–Ω–Ω—ñ –ø–µ—Ä–µ–∫–æ–Ω–∞—Ç–∏—Å—è, —â–æ —Ç–µ—Å—Ç–æ–≤–∏–π –Ω–∞–±—ñ—Ä –Ω–µ –º–∞—î –ø—Ä–æ–ø—É—â–µ–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω—å —É –≤–∞–∂–ª–∏–≤–∏—Ö —Å—Ç–æ–≤–ø—Ü—è—Ö —ñ —â–æ –¥–∞–Ω—ñ –¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è —Ç–∞ –Ω–∞–≤—á–∞–Ω–Ω—è —Å—É–º—ñ—Å–Ω—ñ.
2. –í—ñ–¥—Å—É—Ç–Ω—ñ—Å—Ç—å –º—ñ—Ç–æ–∫ —É —Ç–µ—Å—Ç–æ–≤–æ–º—É –Ω–∞–±–æ—Ä—ñ:

–¢–µ—Å—Ç–æ–≤–∏–π –Ω–∞–±—ñ—Ä –º—ñ—Å—Ç–∏—Ç—å –ª–∏—à–µ id —ñ comment_text, –∞–ª–µ –Ω–µ –º—ñ—Å—Ç–∏—Ç—å –º—ñ—Ç–æ–∫ —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç—ñ. –¶–µ –≤–∫–∞–∑—É—î –Ω–∞ —Ç–µ, —â–æ —Ç–µ—Å—Ç–æ–≤–∏–π –Ω–∞–±—ñ—Ä –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ –º–æ–¥–µ–ª—ñ –ø—ñ—Å–ª—è —ó—ó –Ω–∞–≤—á–∞–Ω–Ω—è –Ω–∞ train, —ñ –º–∏ –Ω–µ –º–∞—î–º–æ —Ñ–∞–∫—Ç–∏—á–Ω–∏—Ö –º—ñ—Ç–æ–∫ –¥–ª—è –æ—Ü—ñ–Ω–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤.
"""

# –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω—å —É —Ç–µ—Å—Ç–æ–≤–æ–º—É –Ω–∞–±–æ—Ä—ñ
print(test.isnull().sum())
print("_____________________________________________")

# –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —ñ–º–µ–Ω —Å—Ç–æ–≤–ø—Ü—ñ–≤
print("–ù–∞–≤—á–∞–ª—å–Ω–∏–π –Ω–∞–±—ñ—Ä:", train.columns)
print("–¢–µ—Å—Ç–æ–≤–∏–π –Ω–∞–±—ñ—Ä:", test.columns)

# –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ç–∏–ø—ñ–≤ –¥–∞–Ω–∏—Ö
print("–¢–∏–ø–∏ –¥–∞–Ω–∏—Ö –Ω–∞–≤—á–∞–ª—å–Ω–æ–≥–æ –Ω–∞–±–æ—Ä—É:\n", train.dtypes)
print("–¢–∏–ø–∏ –¥–∞–Ω–∏—Ö —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä—É:\n", test.dtypes)
print("_____________________________________________")

# –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω—å —É —Å—Ç–æ–≤–ø—Ü—ñ 'id' –¥–ª—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä—É
print("–£–Ω—ñ–∫–∞–ª—å–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è –≤ 'id':", test['id'].unique())

# –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –ø–µ—Ä—à–æ–≥–æ —Ä—è–¥–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤–∏—Ö –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤
print(test['comment_text'].head())
print("_____________________________________________")

# –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ –≤—Å—ñ—Ö —Å—Ç–æ–≤–ø—Ü—ñ–≤
required_columns = ['id', 'comment_text']
missing_columns = [col for col in required_columns if col not in test.columns]
if missing_columns:
    print(f"–í —Ç–µ—Å—Ç–æ–≤–æ–º—É –Ω–∞–±–æ—Ä—ñ –≤—ñ–¥—Å—É—Ç–Ω—ñ —Å—Ç–æ–≤–ø—Ü—ñ: {missing_columns}")
else:
    print("–í—Å—ñ –Ω–µ–æ–±—Ö—ñ–¥–Ω—ñ —Å—Ç–æ–≤–ø—Ü—ñ –ø—Ä–∏—Å—É—Ç–Ω—ñ.")
print("_____________________________________________")

# –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∫—ñ–ª—å–∫–æ—Å—Ç—ñ —Å–∏–º–≤–æ–ª—ñ–≤ –≤ –ø–µ—Ä—à–∏—Ö –∫—ñ–ª—å–∫–æ—Ö –∫–æ–º–µ–Ω—Ç–∞—Ä—è—Ö
print(test['comment_text'].apply(len).describe())

sns.set(color_codes=True)
comment_len = train.comment_text.str.len()
sns.histplot(comment_len, kde=False, bins=20, color="steelblue")

"""–ß–∞—Å—Ç–æ—Ç–∞ –ø–æ–∑–Ω–∞—á–µ–Ω–∏—Ö –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤. 'toxic' –Ω–∞–π—á–∞—Å—Ç—ñ—à—ñ.


"""

train_labels = train[['toxic', 'severe_toxic',
                      'obscene', 'threat', 'insult', 'identity_hate']]
label_count = train_labels.sum()

label_count.plot(kind='bar', title='Labels Frequency', color='steelblue')

"""–î–µ—è–∫—ñ –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ –Ω–∞–ª–µ–∂–∞—Ç—å –æ–¥–Ω–æ—á–∞—Å–Ω–æ –¥–µ–∫—ñ–ª—å–∫–æ–º –∫–ª–∞—Å–∞–º.
–¶–µ –≤–ø–ª–∏–Ω–µ –Ω–∞ —Å–ø–æ—Å—ñ–± –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è –∫–ª–∞—Å—É
"""

train[train["toxic"]==1]

"""–í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è —á–∞—Å—Ç–æ—Ç–∏"""

barWidth = 0.25

bars1 = [sum(train['toxic'] == 1), sum(train['obscene'] == 1), sum(train['insult'] == 1), sum(train['severe_toxic'] == 1),
         sum(train['identity_hate'] == 1), sum(train['threat'] == 1)]
bars2 = [sum(train['toxic'] == 0), sum(train['obscene'] == 0), sum(train['insult'] == 0), sum(train['severe_toxic'] == 0),
         sum(train['identity_hate'] == 0), sum(train['threat'] == 0)]

r1 = np.arange(len(bars1))
r2 = [x + barWidth for x in r1]

plt.bar(r1, bars1, color='steelblue', width=barWidth, label='labeled = 1')
plt.bar(r2, bars2, color='lightsteelblue', width=barWidth, label='labeled = 0')

plt.xlabel('group', fontweight='bold')
plt.xticks([r + barWidth for r in range(len(bars1))], ['Toxic', 'Obscene', 'Insult', 'Severe Toxic', 'Identity Hate',
                                                       'Threat'])
plt.legend()
plt.show()

"""–ó–∞–≥–∞–ª—å–Ω—ñ –≤–∏—Å–Ω–æ–≤–∫–∏:
* –¢–µ—Å—Ç–æ–≤–∏–π –Ω–∞–±—ñ—Ä –≤–∏–≥–ª—è–¥–∞—î –ø—Ä–∞–≤–∏–ª—å–Ω–æ –ø—ñ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–∏–º —ñ —Å—É–º—ñ—Å–Ω–∏–º –∑ –Ω–∞–≤—á–∞–ª—å–Ω–∏–º –Ω–∞–±–æ—Ä–æ–º, –∫—Ä—ñ–º –≤—ñ–¥—Å—É—Ç–Ω–æ—Å—Ç—ñ –º—ñ—Ç–æ–∫ —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç—ñ, —â–æ —î –∑–≤–∏—á–Ω–∏–º –¥–ª—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä—É.

* –û–±—Ä–æ–±–∫–∞ —Ç–µ–∫—Å—Ç—É: –ó–≤–∞–∂–∞—é—á–∏ –Ω–∞ –≤–∞—Ä—ñ–∞—Ü—ñ—ó –≤ –¥–æ–≤–∂–∏–Ω—ñ –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤, –º–æ–∂–µ –±—É—Ç–∏ –¥–æ—Ü—ñ–ª—å–Ω–æ –ø—Ä–æ–≤–µ—Å—Ç–∏ –ø–æ–ø–µ—Ä–µ–¥–Ω—é –æ–±—Ä–æ–±–∫—É —Ç–µ–∫—Å—Ç—É, —â–æ–± –≤–∏—Ä—ñ—à–∏—Ç–∏ –ø–∏—Ç–∞–Ω–Ω—è –∑ –¥–æ–≤–≥–∏–º–∏ –∫–æ–º–µ–Ω—Ç–∞—Ä—è–º–∏.

–ü—Ä–∏–∫–ª–∞–¥ —á–∏—Å—Ç–æ–≥–æ –∫–æ–º–µ–Ω—Ç–∞—Ä—è
"""

print(train.comment_text[0])

"""–¢–æ–∫—Å–∏—á–Ω–∏–π –∫–æ–º–µ–Ω—Ç–∞—Ä, –ø—Ä–∏–∫–ª–∞–¥"""

print(train[train.toxic == 1].iloc[1, 1])

rowsums = train.iloc[:, 2:].sum(axis=1)
temp = train.iloc[:, 2:-1]
train_corr = temp[rowsums > 0]
corr = train_corr.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(corr,
            xticklabels=corr.columns.values,
            yticklabels=corr.columns.values, annot=True, cmap="Blues")

"""–ú–∞—î–º–æ –≤–∏—Å–æ–∫—É –∫–æ—Ä–µ–ª—è—Ü—ñ—é –º—ñ–∂ threat —Ç–∞ insult —Ç–∞ obscene.

–ü–æ–±—É–¥–æ–≤–∞ –≥—Ä–∞—Ñ—ñ–∫—É —É –≤–∏–≥–ª—è–¥—ñ —Ö–º–∞—Ä–∏ —Å–ª—ñ–≤.

–¶–µ–π –ø—ñ–¥—Ö—ñ–¥ –¥–æ–∑–≤–æ–ª–∏—Ç—å –≤–∞–º –≤—ñ–∑—É–∞–ª—ñ–∑—É–≤–∞—Ç–∏ –Ω–∞–π–ø–æ–ø—É–ª—è—Ä–Ω—ñ—à—ñ —Å–ª–æ–≤–∞ –≤ –∫–æ–º–µ–Ω—Ç–∞—Ä—è—Ö —ñ –º–æ–∂–µ –¥–æ–ø–æ–º–æ–≥—Ç–∏ —É —Ä–æ–∑—É–º—ñ–Ω–Ω—ñ –Ω–∞–π–±—ñ–ª—å—à –≤–∂–∏–≤–∞–Ω–∏—Ö —Ç–µ—Ä–º—ñ–Ω—ñ–≤ —É –≤–∞—à–æ–º—É –Ω–∞–±–æ—Ä—ñ –¥–∞–Ω–∏—Ö.
"""

def W_Cloud(token):
    """
    Visualize the most common words contributing to the token.
    """
    threat_context = train[train[token] == 1]
    threat_text = threat_context.comment_text
    neg_text = pd.Series(threat_text).str.cat(sep=' ')
    wordcloud = WordCloud(width=1600, height=800,
                          max_font_size=200).generate(neg_text)

    plt.figure(figsize=(15, 10))
    plt.imshow(wordcloud.recolor(colormap="Blues"), interpolation='bilinear')
    plt.axis("off")
    plt.title(f"Most common words assosiated with {token} comment", size=20)
    plt.show()

train_labels_words = ['toxic', 'severe_toxic',
                      'obscene', 'threat', 'insult', 'identity_hate']
for token in train_labels_words:
  W_Cloud(token.lower())

"""## Feature-engineering"""

test_labels = ["toxic", "severe_toxic", "obscene",
               "threat", "insult", "identity_hate"]

def tokenize(text):
    '''
    Tokenize text and return a non-unique list of tokenized words found in the text.
    Normalize to lowercase, strip punctuation, remove stop words, filter non-ascii characters.
    Lemmatize the words and lastly drop words of length < 3.
    '''
    text = text.lower()
    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\r\\t\\n]')
    nopunct = regex.sub(" ", text)
    words = nopunct.split(' ')
    # remove any non ascii
    words = [word.encode('ascii', 'ignore').decode('ascii') for word in words]
    lmtzr = WordNetLemmatizer()
    words = [lmtzr.lemmatize(w) for w in words]
    words = [w for w in words if len(w) > 2]
    return words

vector = TfidfVectorizer(ngram_range=(1, 1), analyzer='word',
                         tokenizer=tokenize, stop_words='english',
                         strip_accents='unicode', use_idf=1, min_df=10)
X_train = vector.fit_transform(train['comment_text'])
X_test = vector.transform(test['comment_text'])

vector.get_feature_names_out()[0:20]

df_test = pd.merge(test, test_y, on="id")

df_test.head()

df_test.shape

test.head()

"""–¶–µ–π –ø—Ä–æ—Ü–µ—Å —Å—Ç–≤–æ—Ä–∏–≤ –≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω—ñ –¥–∞–Ω—ñ, –≥–æ—Ç–æ–≤—ñ –¥–æ –ø–æ–¥–∞–ª—å—à–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ"""